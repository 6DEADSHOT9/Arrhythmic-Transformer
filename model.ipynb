{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "listed\n",
      "ragged\n"
     ]
    }
   ],
   "source": [
    "NORM = np.load('Heartbeats/Normal/N_beats.npy',allow_pickle = True)\n",
    "ARRH = np.load('Heartbeats/Arrhythmia/A_beats.npy',allow_pickle = True)\n",
    "\n",
    "print('loaded')\n",
    "NORM = list(NORM)\n",
    "ARRH = list(ARRH)\n",
    "print('listed')\n",
    "\n",
    "NORM = tf.ragged.constant(NORM)\n",
    "ARRH = tf.ragged.constant(ARRH)\n",
    "print('ragged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrh_labels = np.array(tf.cast(np.array(np.ones(int(ARRH.shape[0]))),tf.float16))\n",
    "norm_labels = np.array(tf.cast(np.array(np.zeros(int(NORM.shape[0]))),tf.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "all_data = tf.concat([NORM,ARRH],axis = 0)\n",
    "all_labels = np.concatenate([norm_labels,arrh_labels],axis = 0)\n",
    "# all_labels = list(all_labels)\n",
    "# all_data = all_data.to_list()\n",
    "print(type(all_data),type(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((all_data,all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (500**depths)         # (1, depth)\n",
    "    angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.sin(angle_rads)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.positional_encoding = positional_encoding(length=875, depth=d_model).numpy()\n",
    "\n",
    "    def call(self, x):\n",
    "        # length = tf.shape(x)[1]\n",
    "        # temp = input_embeddings(one_hot(x,d_model),875,d_model)\n",
    "        x = x + self.positional_encoding[tf.newaxis, :, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),\n",
    "          tf.keras.layers.Dense(d_model),\n",
    "          tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.add([x, self.seq(x)])\n",
    "        x = self.layer_norm(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(d_model=d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 512\n",
    "dff = 2048\n",
    "input_vocab_size = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Equinox(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, dropout_rate=0.1):#, target_vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "    \n",
    "        self.dense1 = tf.keras.layers.Dense(256)\n",
    "        self.dense2 = tf.keras.layers.Dense(128)\n",
    "        self.final_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "#         context, x  = inputs\n",
    "#         context = self.encoder(context)\n",
    "#         print(context,x)\n",
    "        \n",
    "        x = self.encoder(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        logits = self.final_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"equinox_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_4 (Encoder)         multiple                  63019008  \n",
      "                                                                 \n",
      " dense_180 (Dense)           multiple                  131328    \n",
      "                                                                 \n",
      " dense_181 (Dense)           multiple                  32896     \n",
      "                                                                 \n",
      " dense_182 (Dense)           multiple                  129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,183,361\n",
      "Trainable params: 63,183,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Equinox_M = Equinox(num_layers = 6,d_model = 512,dff = 2048,input_vocab_size = 512,num_heads = 8,dropout_rate = 0.1)\n",
    "# Equinox_M.build(input_shape = (875,512))\n",
    "Equinox_M.compile(optimizer = tf.keras.optimizers.Adam(),metrics = ['accuracy'])\n",
    "# Equinox_M.build(input_shape = (875,512))\n",
    "Equinox_M(np.ones((875,512)))\n",
    "Equinox_M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Equinox_M.fit(train_ds.batch(batch_size),epochs=epochs,validation_data=val_ds.batch(batch_size),shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Arrhythmia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
